{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nbCount : 20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n#Ou sinon\\nquery = input(\"Enter keys words...\")\\nresults = search_engine.search(query)\\nprint(results)\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import requests\n",
    "from datetime import datetime, timezone \n",
    "from Classes import Document, Author, HackerNewsDocument, TheGuardianDocument\n",
    "from bs4 import BeautifulSoup\n",
    "from Corpus import Corpus, DocumentFactory\n",
    "from scipy.sparse import csr_matrix\n",
    "from SearchEngine import SearchEngine\n",
    "\n",
    "#--------------D√©finition des variables\n",
    "#variable pour stocker les documents √† l'√©tat 'brut'\n",
    "collection = []\n",
    "#Nombre d'articles √† r√©cup√©rer\n",
    "nbDoc = 10\n",
    "#query = [\"Day\",\"Country\",\"Travel\",\"Tokyo\"] \n",
    "query =\"War\"\n",
    "api_key_guardian = \"265a16e3-294c-4c62-ae88-a274906a6333\"\n",
    "\n",
    "\n",
    "def search_query(texte,mots_cles) : \n",
    "    #V√©rifie si un des mots cl√©s est pr√©sent dans le texte ou le titre\n",
    "    return any(mot.lower() in texte.lower() for mot in mots_cles)\n",
    "\n",
    "def extraire_text_url(url) : \n",
    "    try:\n",
    "        #On r√©cup√®re la page html puis on v√©rifie si la requ√™te a r√©ussi\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        #On extrait uniquement les balises <p> pr√©sentes dans le body\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        body = soup.body\n",
    "        if body is None:\n",
    "            return \"Texte non disponible\" #Car pas de body\n",
    "           \n",
    "        paragraphes = body.find_all(\"p\")    #Texte √† partir du body\n",
    "        texte = \"\\n\".join(p.get_text(strip=True) for p in paragraphes)\n",
    "        if(texte.strip) : \n",
    "            return texte\n",
    "        else : \n",
    "            return \"Texte non disponible\"\n",
    "    except Exception :\n",
    "        return \"Texte non disponible\"\n",
    "\n",
    "#-----------------------WebScrapping avec Hacker News API\n",
    "def add_doc_HackerNews(collection,query,nbDoc) :\n",
    "    #Nombre de storys r√©cup√©r√©es\n",
    "    nbCount =0\n",
    "    url = \"https://hacker-news.firebaseio.com/v0/beststories.json\"\n",
    "    response = requests.get(url)\n",
    "    #Renvoie une exception si aucun article n'a √©t√© trouv√© \n",
    "    if response.status_code !=200 : \n",
    "        raise Exception(f\"Aucun texte provenant de HackerNews ne correspond √† la recherche {query}\")\n",
    "    \n",
    "    top_stories = response.json()\n",
    "    for id in top_stories[:1000]:  #C'est pour √™tre s√ªr d'avoir un jeu de donn√©es cons√©quent\n",
    "        #Si on a atteint le nombre de doc, on arr√™te de chercher\n",
    "        if nbCount >= nbDoc:\n",
    "            break\n",
    "\n",
    "        url = f\"https://hacker-news.firebaseio.com/v0/item/{id}.json\"\n",
    "        \n",
    "        data = requests.get(url)\n",
    "        data = data.json()\n",
    "        #R√©cup√©ration des donn√©es pour chaque url\n",
    "        titre = data.get(\"title\", \"No title\")\n",
    "        auteur = data.get(\"by\", \"Unknown\")\n",
    "        timestamp = data.get(\"time\", 0)\n",
    "        #Formatage de la date \n",
    "        date = datetime.fromtimestamp(timestamp, tz=timezone.utc).strftime('%Y-%m-%d %H:%M:%S')\n",
    "        article_url = data.get(\"url\")\n",
    "        texte = extraire_text_url(article_url)  \n",
    "        score = data.get(\"score\", 0)\n",
    "\n",
    "        #On cr√©√© un document √† partir des informations r√©colt√©es\n",
    "        if texte != \"Texte non disponible\" :\n",
    "            #On applique la recherche de mots cl√©s sur le titre et le texte\n",
    "            if search_query(titre, query) or search_query(texte, query):\n",
    "                doc = DocumentFactory.creerDoc('HackerNews', titre, auteur, date, article_url, texte, score)\n",
    "                collection.append(doc)\n",
    "                nbCount +=1\n",
    "    return collection\n",
    "\n",
    "#-----------------------WebScrapping avec The Guardian API\n",
    "def add_doc_Guardian(collection, query, nbDoc, api_key):\n",
    "    nbCount = 0\n",
    "    url = f\"https://content.guardianapis.com/search?q={query}&page-size={nbDoc}&api-key={api_key}\"\n",
    "    response = requests.get(url)\n",
    "    #Renvoie une exception si aucun article n'a √©t√© trouv√© \n",
    "    if response.status_code !=200 : \n",
    "        raise Exception(f\"Aucun article provenant de The Guardian ne correspond √† la recherche {query}\")\n",
    "    \n",
    "    data = response.json()\n",
    "    for article in data[\"response\"][\"results\"]:\n",
    "        #Si on a atteint le nombre de doc, on arr√™te de chercher\n",
    "        if nbCount >= nbDoc:\n",
    "            break\n",
    "        #R√©cup√©ration des donn√©es pour chaque url\n",
    "        titre = article[\"webTitle\"]\n",
    "        article_url = article[\"webUrl\"]\n",
    "        texte = extraire_text_url(article_url)\n",
    "        try :   #Il n'y a pas tout le temps des auteurs\n",
    "            auteur = article.get(\"author\", \"Auteur inconnu\")\n",
    "        except : \n",
    "            auteur = \"The Guardian\"\n",
    "        # Premi√®re date de publication (on r√©cup√®re sous forme de string avant de la convertir en date)\n",
    "        release_date_str = article.get(\"firstPublicationDate\", \"\")\n",
    "        release_date = None     #Il faut instancier\n",
    "        if release_date_str:\n",
    "            release_date = datetime.strptime(release_date_str, \"%Y-%m-%dT%H:%M:%S.%fZ\")  \n",
    "        \n",
    "        # Derni√®re date de mise √† jour (on r√©cup√®re sous forme de string avant de la convertir en date)\n",
    "        last_maj_str = article.get(\"lastModified\", \"\")\n",
    "        last_maj = None         #Il faut instancier\n",
    "        if last_maj_str:\n",
    "            last_maj = datetime.strptime(last_maj_str, \"%Y-%m-%dT%H:%M:%S.%fZ\") \n",
    "\n",
    "        if texte != \"Texte non disponible\":\n",
    "            doc = DocumentFactory.creerDoc('The_Guardian', titre, auteur, last_maj, article_url, texte, release_date)\n",
    "            collection.append(doc)\n",
    "            nbCount += 1\n",
    "\n",
    "    return collection\n",
    "\n",
    "#-------------------RECUPERATION DES DONNEES\n",
    "add_doc_HackerNews(collection,query,nbDoc) \n",
    "add_doc_Guardian(collection,query, nbDoc, api_key_guardian)\n",
    "'''\n",
    "for doc in collection : \n",
    "    print(doc.texte)\n",
    "    print('------------------------------')\n",
    "'''\n",
    "print(f\"nbCount : {len(collection)}\")    \n",
    "\n",
    "\n",
    "# Cr√©ation de l'index de documents √† partir de la collection\n",
    "#Cl√© : un Id \n",
    "#Valeur : le titre du document\n",
    "id2doc = {}\n",
    "for i, doc in enumerate(collection):\n",
    "    id2doc[i] = doc.titre\n",
    "\n",
    "authors = {}\n",
    "aut2id = {}\n",
    "num_auteurs_vus = 0\n",
    "\n",
    "# Cr√©ation de l'index des Auteurs √† partir de la collection\n",
    "#Cl√© : un Id (en fonction de la valeur de la variable num_auteurs_vus)\n",
    "#Valeur : un objet de type Author\n",
    "for doc in collection:\n",
    "    if doc.auteur not in aut2id:\n",
    "        num_auteurs_vus += 1\n",
    "        authors[num_auteurs_vus] = Author(doc.auteur)\n",
    "        aut2id[doc.auteur] = num_auteurs_vus\n",
    "    authors[aut2id[doc.auteur]].add(doc.texte)\n",
    "\n",
    "\n",
    "# Construction du corpus √† partir des documents pr√©sents dans la collection\n",
    "corpus = Corpus(\"Mon corpus\")\n",
    "for doc in collection:\n",
    "    corpus.add(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cc15e96955c481ca13f54d9be835337",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=\"<h2 style='color: lightpink; text-align: center;'>üïµüèΩ\\u200d‚ôÄÔ∏è Mon moteur de recherch‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4999 mots diff√©rents dans le vocabulaire\n",
      "Les 10 mots les plus fr√©quents :\n",
      "      Mot    TF  DF\n",
      "13    the  1070  16\n",
      "44    and   655  16\n",
      "18     to   540  16\n",
      "3      of   534  16\n",
      "32      a   533  16\n",
      "28     in   383  16\n",
      "166    on   301  15\n",
      "244   for   190  15\n",
      "319    is   180  15\n",
      "24   that   174  16\n"
     ]
    }
   ],
   "source": [
    "import ipywidgets as wg\n",
    "from IPython.display import display\n",
    "from SearchEngine import SearchEngine\n",
    "\n",
    "label = wg.HTML(\n",
    "    \"<h2 style='color: lightpink; text-align: center;'>üïµüèΩ‚Äç‚ôÄÔ∏è Mon moteur de recherche üîé</h2>\"\n",
    ")\n",
    "\n",
    "text_widget = wg.Text(\n",
    "    placeholder=\"Entrez vos mots ici\",\n",
    "    layout=wg.Layout(width='300px')\n",
    ")\n",
    "\n",
    "text_input = wg.HBox([\n",
    "    wg.HTML(\n",
    "        \"<b style='font-size: 16px; color: lightgreen;'>Mots cl√©s :</b>\"\n",
    "    ),\n",
    "    text_widget\n",
    "])\n",
    "\n",
    "slider = wg.HBox([\n",
    "    wg.HTML(\n",
    "        \"<b style='font-size: 12px; color: grey;'>Nb de documents :</b>\"\n",
    "    ),\n",
    "    wg.IntSlider(\n",
    "        value=5, \n",
    "        min=1, \n",
    "        max=20, \n",
    "        step=1,\n",
    "        layout=wg.Layout(width='200px')  \n",
    "    )\n",
    "])\n",
    "\n",
    "search_button = wg.Button(\n",
    "    description=\"Rechercher\", \n",
    "    icon=\"search\",\n",
    "    style={'button_color': '#FFDAB9'}\n",
    ")\n",
    "\n",
    "output = wg.Output()\n",
    "\n",
    "def search_corpus(corpus, query, max_docs):\n",
    "    search_engine = SearchEngine(corpus)\n",
    "    results = []\n",
    "    with output:\n",
    "        output.clear_output() # effacer la sortie pr√©c√©dente\n",
    "        try:\n",
    "            result_generator = search_engine.search(query)  \n",
    "            print(result_generator)\n",
    "            for i, excerpt in enumerate(result_generator):\n",
    "                if i >= max_docs:  # Limiter le nombre de r√©sultats\n",
    "                    break\n",
    "                results.append(excerpt)\n",
    "            if not results:\n",
    "                print(f\"Aucun r√©sultat trouv√© pour la requ√™te : '{query}'\")\n",
    "        except Exception as e:\n",
    "            print(\"Erreur lors de la recherche :\", str(e))\n",
    "            return\n",
    "\n",
    "def clique_bouton(b) :\n",
    "    query = text_widget.value\n",
    "    num_docs = slider.children[1].value\n",
    "    with output :\n",
    "        output.clear_output()\n",
    "        if not query.strip() :\n",
    "            return\n",
    "    search_corpus(corpus, query, num_docs)\n",
    "\n",
    "search_button.on_click(clique_bouton)\n",
    "\n",
    "interface = wg.VBox([label, text_input, slider, search_button, output],\n",
    "        layout=wg.Layout(justify_content=\"center\", \n",
    "        align_items=\"center\",         \n",
    "        padding=\"20px\",\n",
    "        height= '300px'))\n",
    "display(interface)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
